{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import signal\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import common as util\n",
    "from buffer import ActionTraceBuf\n",
    "from myenv import Env\n",
    "from networks.adrqn_network import Qnetwork\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sess, mainQN, env_name, skip=6, scenario_count=3, is_render=False):\n",
    "    start_time = time.time()\n",
    "    env = Env(env_name=env_name, skip=skip)\n",
    "\n",
    "    def total_scenario_reward():\n",
    "        (s, R, _), t, state = env.reset(), False, None\n",
    "        while not t:\n",
    "            action, state = mainQN.get_action_and_next_state(sess, state, [s])\n",
    "            s, r, t, _ = env.step(action)\n",
    "            R += r\n",
    "            if is_render:\n",
    "                env.render()\n",
    "        return R\n",
    "\n",
    "    res = np.array([total_scenario_reward() for _ in range(scenario_count)])\n",
    "    print(time.time() - start_time, 'seconds to evaluate', flush=1)\n",
    "    return np.mean(res), np.std(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, ?, 1024)\n",
      "(?, 256)\n",
      "(?, ?, 1024)\n",
      "(?, 256)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Exiting = 0\n",
    "trace_length =10\n",
    "render_eval=False\n",
    "h_size=512\n",
    "action_h_size=512\n",
    "target_update_freq=10000\n",
    "ckpt_freq=500000\n",
    "summary_freq=1000\n",
    "eval_freq=10000\n",
    "batch_size=32\n",
    "env_name='SpaceInvaders'\n",
    "total_iteration=5e7\n",
    "pretrain_steps=50000\n",
    "\n",
    "    \n",
    "# env_name += 'NoFrameskip-v4'\n",
    "identity = 'stack={},env={},mod={}'.format(trace_length, env_name, 'adrqn')\n",
    "\n",
    "env = Env(env_name=env_name, skip=4)\n",
    "a_size = env.n_actions\n",
    "\n",
    "tf.reset_default_graph()\n",
    "cell = tf.nn.rnn_cell.LSTMCell(num_units=h_size)\n",
    "cellT = tf.nn.rnn_cell.LSTMCell(num_units=h_size)\n",
    "mainQN = Qnetwork(h_size, a_size, action_h_size, cell, 'main')\n",
    "targetQN = Qnetwork(h_size, a_size, action_h_size, cellT, 'target')\n",
    "init = tf.global_variables_initializer()\n",
    "updateOps = util.getTargetUpdateOps(tf.trainable_variables())\n",
    "saver = tf.train.Saver(max_to_keep=5)\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "sess.run(init)\n",
    "summary_writer = tf.summary.FileWriter('./log/' + identity, sess.graph)\n",
    "\n",
    "if util.checkpoint_exists(identity):\n",
    "    (exp_buf, env, last_iteration, is_done,\n",
    "     prev_life_count, action, state, S) = util.load_checkpoint(sess, saver, identity)\n",
    "    start_time = time.time()\n",
    "else:\n",
    "    exp_buf = ActionTraceBuf(trace_length, scenario_size=2500)\n",
    "    last_iteration = 1 - pretrain_steps\n",
    "    is_done = True\n",
    "    action = 0\n",
    "    prev_life_count = None\n",
    "    state = None\n",
    "    sess.run(updateOps)\n",
    "\n",
    "summaryOps = tf.summary.merge_all()\n",
    "\n",
    "eval_summary_ph = tf.placeholder(tf.float32, shape=(2,), name='evaluation')\n",
    "evalOps = (tf.summary.scalar('performance', eval_summary_ph[0]),\n",
    "           tf.summary.scalar('perform_std', eval_summary_ph[1]))\n",
    "online_summary_ph = tf.placeholder(tf.float32, shape=(2,), name='online')\n",
    "onlineOps = (tf.summary.scalar('online_performance', online_summary_ph[0]),\n",
    "             tf.summary.scalar('online_scenario_length', online_summary_ph[1]))\n",
    "\n",
    "for i in range(last_iteration, int(total_iteration)):\n",
    "    if is_done:\n",
    "        total_scenario_reward = exp_buf.get_cache_total_reward()\n",
    "        if i > 0:\n",
    "            online_perf_and_length = np.array(\n",
    "                [total_scenario_reward, len(exp_buf.trans_cache)])\n",
    "            online_perf, online_episode_count = sess.run(onlineOps, feed_dict={\n",
    "                online_summary_ph: online_perf_and_length})\n",
    "            summary_writer.add_summary(online_perf, i)\n",
    "            summary_writer.add_summary(online_episode_count, i)\n",
    "        exp_buf.flush_scenario()\n",
    "        s, r, prev_life_count = env.reset()\n",
    "        S = [s]\n",
    "        action, state = mainQN.get_action_and_next_state(sess, None, [action], S)\n",
    "\n",
    "    S = [S[-1]]\n",
    "    for _ in range(4):\n",
    "        s, r, is_done, life_count = env.step(action)\n",
    "        exp_buf.append_trans((\n",
    "            S[-1], action, r, s,  # not cliping reward (huber loss)\n",
    "            (prev_life_count and life_count < prev_life_count or is_done)\n",
    "        ))\n",
    "        S.append(s)\n",
    "        prev_life_count = life_count\n",
    "\n",
    "    action, state = mainQN.get_action_and_next_state(sess, state, [action]*len(S), S)\n",
    "    if np.random.random() < util.epsilon_at(i):\n",
    "        action = env.rand_action()\n",
    "\n",
    "    if not i:\n",
    "        start_time = time.time()\n",
    "\n",
    "    if i <= 0:\n",
    "        continue\n",
    "\n",
    "    if Exiting or not i % ckpt_freq:\n",
    "        util.checkpoint(sess, saver, identity,\n",
    "                   exp_buf, env, i, is_done,\n",
    "                   prev_life_count, action, state, S)\n",
    "        if i % ckpt_freq:\n",
    "            raise SystemExit\n",
    "\n",
    "    if not i % target_update_freq:\n",
    "        sess.run(updateOps)\n",
    "        cur_time = time.time()\n",
    "        print('[{}{}:{}] took {} seconds to {} steps'.format(\n",
    "            'ADRQN', trace_length, i, cur_time-start_time, target_update_freq), flush=1)\n",
    "        start_time = cur_time\n",
    "\n",
    "    #ã€€TRAINING STARTS\n",
    "    state_train = (np.zeros((batch_size, h_size)),) * 2\n",
    "\n",
    "    trainBatch = exp_buf.sample_traces(batch_size)\n",
    "\n",
    "    Q1 = sess.run(mainQN.predict, feed_dict={\n",
    "        mainQN.scalarInput: np.vstack(trainBatch[:, 3]/255.0),\n",
    "        targetQN.actionsInput: trainBatch[:, 1],\n",
    "        mainQN.trainLength: trace_length,\n",
    "        mainQN.state_init: state_train,\n",
    "        mainQN.batch_size: batch_size\n",
    "    })\n",
    "    Q2 = sess.run(targetQN.Qout, feed_dict={\n",
    "        targetQN.scalarInput: np.vstack(trainBatch[:, 3]/255.0),\n",
    "        targetQN.actionsInput: trainBatch[:, 1],\n",
    "        targetQN.trainLength: trace_length,\n",
    "        targetQN.state_init: state_train,\n",
    "        targetQN.batch_size: batch_size\n",
    "    })\n",
    "    end_multiplier = - (trainBatch[:, 4] - 1)\n",
    "    doubleQ = Q2[range(batch_size * trace_length), Q1]\n",
    "    targetQ = trainBatch[:, 2] + (0.99 * doubleQ * end_multiplier)\n",
    "\n",
    "    # print(targetQ.shape)\n",
    "    _, summary = sess.run((mainQN.updateModel, summaryOps), feed_dict={\n",
    "        mainQN.scalarInput: np.vstack(trainBatch[:, 0]/255.0),\n",
    "        mainQN.actionsInput: trainBatch[:, 5],\n",
    "        mainQN.targetQ: targetQ,\n",
    "        mainQN.actions: trainBatch[:, 1],\n",
    "        mainQN.trainLength: trace_length,\n",
    "        mainQN.state_init: state_train,\n",
    "        mainQN.batch_size: batch_size\n",
    "    })\n",
    "\n",
    "    if not i % summary_freq:\n",
    "        summary_writer.add_summary(summary, i)\n",
    "    if not i % eval_freq:\n",
    "        eval_res = np.array(\n",
    "            evaluate(sess, mainQN, env_name, is_render=render_eval))\n",
    "        perf, perf_std = sess.run(\n",
    "            evalOps, feed_dict={eval_summary_ph: eval_res})\n",
    "        summary_writer.add_summary(perf, i)\n",
    "        summary_writer.add_summary(perf_std, i)\n",
    "# In the end\n",
    "sess.close()\n",
    "util.checkpoint(sess, saver, identity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'exp_buf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-aa8f4700ccb5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mexp_buf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'exp_buf' is not defined"
     ]
    }
   ],
   "source": [
    "exp_buf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_buf.sample_traces(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
